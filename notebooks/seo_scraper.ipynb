{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c68c4c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: altair in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from -r requirements.txt (line 1)) (5.5.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from -r requirements.txt (line 2)) (2.0.3)\n",
      "Requirement already satisfied: streamlit in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from -r requirements.txt (line 3)) (1.45.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from altair->-r requirements.txt (line 1)) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from altair->-r requirements.txt (line 1)) (4.24.0)\n",
      "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from altair->-r requirements.txt (line 1)) (1.41.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from altair->-r requirements.txt (line 1)) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from altair->-r requirements.txt (line 1)) (4.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (2025.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from pandas->-r requirements.txt (line 2)) (1.23.5)\n",
      "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from streamlit->-r requirements.txt (line 3)) (1.9.0)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from streamlit->-r requirements.txt (line 3)) (5.5.2)\n",
      "Requirement already satisfied: click<9,>=7.0 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from streamlit->-r requirements.txt (line 3)) (8.2.1)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from streamlit->-r requirements.txt (line 3)) (11.2.1)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from streamlit->-r requirements.txt (line 3)) (4.25.8)\n",
      "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from streamlit->-r requirements.txt (line 3)) (20.0.0)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from streamlit->-r requirements.txt (line 3)) (2.32.3)\n",
      "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from streamlit->-r requirements.txt (line 3)) (9.1.2)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from streamlit->-r requirements.txt (line 3)) (0.10.2)\n",
      "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from streamlit->-r requirements.txt (line 3)) (6.0.0)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from streamlit->-r requirements.txt (line 3)) (3.1.44)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from streamlit->-r requirements.txt (line 3)) (0.9.1)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from streamlit->-r requirements.txt (line 3)) (6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from click<9,>=7.0->streamlit->-r requirements.txt (line 3)) (0.4.6)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 3)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit->-r requirements.txt (line 3)) (5.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 3)) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 3)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 3)) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from requests<3,>=2.27->streamlit->-r requirements.txt (line 3)) (2025.4.26)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from jinja2->altair->-r requirements.txt (line 1)) (2.1.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from jsonschema>=3.0->altair->-r requirements.txt (line 1)) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from jsonschema>=3.0->altair->-r requirements.txt (line 1)) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from jsonschema>=3.0->altair->-r requirements.txt (line 1)) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from jsonschema>=3.0->altair->-r requirements.txt (line 1)) (0.25.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\moham\\anaconda3\\envs\\seo_final\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28b6fd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries loaded successfully!\n",
      "\n",
      "Scraping: https://en.wikipedia.org/wiki/Web_scraping\n",
      "Scraped 26551 chars\n",
      "\n",
      "Scraping: https://httpbin.org/status/404\n",
      "Attempt 1 failed: HTTPError\n",
      "Attempt 2 failed: HTTPError\n",
      "Attempt 3 failed: HTTPError\n",
      "Failed: 404 Client Error: NOT FOUND for url: https://httpbin.org/status/404\n",
      "\n",
      "Scraping: https://www.nytimes.com\n",
      "Scraped 8845 chars\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>url</th>\n",
       "      <th>chars</th>\n",
       "      <th>last_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>success</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Web_scraping</td>\n",
       "      <td>26551.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>failed</td>\n",
       "      <td>https://httpbin.org/status/404</td>\n",
       "      <td>NaN</td>\n",
       "      <td>404 Client Error: NOT FOUND for url: https://h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>success</td>\n",
       "      <td>https://www.nytimes.com</td>\n",
       "      <td>8845.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    status                                         url    chars  \\\n",
       "0  success  https://en.wikipedia.org/wiki/Web_scraping  26551.0   \n",
       "1   failed              https://httpbin.org/status/404      NaN   \n",
       "2  success                     https://www.nytimes.com   8845.0   \n",
       "\n",
       "                                          last_error  \n",
       "0                                                NaN  \n",
       "1  404 Client Error: NOT FOUND for url: https://h...  \n",
       "2                                                NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# *******\n",
    "# IMPORTS\n",
    "# *******\n",
    "try:\n",
    "    import requests  \n",
    "    from bs4 import BeautifulSoup  \n",
    "    import pandas as pd  \n",
    "    import time  \n",
    "    \n",
    "    # User agent rotation\n",
    "    try:\n",
    "        from fake_useragent import UserAgent  \n",
    "        ua = UserAgent()\n",
    "        headers = {'User-Agent': ua.chrome}\n",
    "    except:\n",
    "        # Fallback\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}\n",
    "    \n",
    "    print(\"All libraries loaded successfully!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"Critical import missing: {e}\")\n",
    "    print(\"Run: pip install requests beautifulsoup4 pandas fake-useragent\")\n",
    "\n",
    "# ***************************\n",
    "# PRODUCTION SCRAPER FUNCTION\n",
    "# ***************************\n",
    "def scraper(url, timeout=10, retries=3):\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            # Make HTTP request (with timeout to prevent hanging)\n",
    "            response = requests.get(\n",
    "                url,\n",
    "                headers=headers,\n",
    "                timeout=timeout\n",
    "            )\n",
    "            \n",
    "            # Verify successful response (raises HTTPError for 4XX/5XX)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse HTML with BeautifulSoup\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Remove unwanted elements\n",
    "            for element in soup(['script', 'style', 'iframe', 'nav', 'footer']):\n",
    "                element.decompose()\n",
    "                \n",
    "            # Extract clean text\n",
    "            text = soup.get_text(separator=' ', strip=True)\n",
    "            \n",
    "            # Add delay (avoids getting blocked)\n",
    "            time.sleep(1.5)\n",
    "            \n",
    "            # Return structured success result\n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'url': url,\n",
    "                'text': text,\n",
    "                'chars': len(text),\n",
    "                'words': len(text.split()),\n",
    "                'attempt': attempt + 1\n",
    "            }\n",
    "            \n",
    "        # ERROR HANDLING \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            last_error = str(e)  # Store the error\n",
    "            print(f\"Attempt {attempt + 1} failed: {type(e).__name__}\")\n",
    "            time.sleep(2 ** attempt)\n",
    "    \n",
    "    # Return structured failure if all retries exhausted\n",
    "    return {\n",
    "        'status': 'failed',\n",
    "        'url': url,\n",
    "        'error': f\"All {retries} attempts failed\",\n",
    "        'last_error': last_error  \n",
    "    }\n",
    "\n",
    "# ******************\n",
    "# TESTING FRAMEWORK\n",
    "# ******************\n",
    "if __name__ == \"__main__\":\n",
    "    # Test URLs covering different scenarios\n",
    "    test_urls = [\n",
    "    \"https://en.wikipedia.org/wiki/Web_scraping\",\n",
    "    \"https://httpbin.org/status/404\",\n",
    "    \"https://www.nytimes.com\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "for url in test_urls:\n",
    "    print(f\"\\nScraping: {url}\")\n",
    "    result = scraper(url)\n",
    "    results.append(result)\n",
    "    \n",
    "    if result['status'] == 'success':\n",
    "        print(f\"Scraped {result['chars']} chars\")\n",
    "    else:\n",
    "        print(f\"Failed: {result['last_error']}\")\n",
    "\n",
    "pd.DataFrame(results)[['status', 'url', 'chars', 'last_error']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac22fa80-f3ac-4c0c-9176-9b545222a578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
